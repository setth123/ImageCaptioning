{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "The goal of this project is to develop a multimodal model that combines ResNet50, a powerful deep neural network for feature extraction from images, and GPT-2, an advanced language model, to generate captions for images. We will be utilizing the Flickr30k dataset in English, a large image dataset paired with textual descriptions, to train and fine-tune the model. </br>\n",
        "Specifically, the ResNet50 model will be used to extract features from the images, while GPT-2 will be fine-tuned to generate relevant captions based on these extracted features"
      ],
      "metadata": {
        "id": "pGz2GG-_y1nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracr feature using ResNet50"
      ],
      "metadata": {
        "id": "-B6znB8Jn1ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "# change to gpu if it avaiable\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#load pretrained ResNet\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "resnet = resnet.to(device).eval()\n",
        "\n",
        "#processing image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "#extract features\n",
        "def extract_image_features(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        features = resnet(image_tensor).squeeze(-1).squeeze(-1)  # [2048]\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "iEczJbAdoGon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize dataset"
      ],
      "metadata": {
        "id": "VuZA84cYrORf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "#tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#tokenizing the dataset\n",
        "from torch.utils.data import Dataset\n",
        "class ImageCaptionDataset(Dataset):\n",
        "  def __init__(self, image_dir, caption_file):\n",
        "        self.image_dir = image_dir\n",
        "        self.samples = []\n",
        "        with open(caption_file, 'r') as f:\n",
        "            for line in f:\n",
        "                img_name, caption = line.strip().split('|')\n",
        "                self.samples.append((img_name, caption))\n",
        "  def __len__(self):\n",
        "        return len(self.samples)\n",
        "  def __getitem__(self, idx):\n",
        "        img_name, caption = self.samples[idx]\n",
        "        image_path = os.path.join(self.image_dir, img_name)\n",
        "        img_feat = extract_image_features(image_path)\n",
        "\n",
        "        tokens = tokenizer(caption, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=30)\n",
        "        return img_feat, tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "WMXUAaBnrRR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine resnet and gpt2"
      ],
      "metadata": {
        "id": "MF7I2j8atdpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "class ImageCaptioningModel(nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt2.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        self.image_proj = nn.Linear(2048, self.gpt2.config.n_embd)\n",
        "        self.prefix_length = 1\n",
        "\n",
        "  def forward(self, image_features, input_ids, attention_mask):\n",
        "        batch_size = image_features.size(0)\n",
        "        #embedding\n",
        "        prefix_embeddings = self.image_proj(image_features).unsqueeze(1)  # (B, 1, D)\n",
        "        gpt_embeddings = self.gpt2.transformer.wte(input_ids)\n",
        "        full_embeddings = torch.cat([prefix_embeddings, gpt_embeddings], dim=1)\n",
        "        #\n",
        "        extended_attention_mask = torch.cat([torch.ones((batch_size, self.prefix_length), device=image_features.device), attention_mask], dim=1)\n",
        "\n",
        "        outputs = self.gpt2(inputs_embeds=full_embeddings, attention_mask=extended_attention_mask, labels=input_ids)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "eGbGGQlGth56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "FfsXCOOOuJf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "dataPath = '/content/drive/My Drive/Dataset/flickrDataset'\n",
        "savePath='/content/drive/My Drive/AI Models/ImageCaptioning'\n",
        "checkpoint_path = f\"{savePath}/caption_checkpoint.pth\"\n",
        "\n",
        "# Dataset + Dataloader\n",
        "dataset = ImageCaptionDataset(f\"{dataPath}/flickr30k_images\", f\"{dataPath}/result.csv\")\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Model + Optimizer\n",
        "model = ImageCaptioningModel().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# === Load checkpoint if available ===\n",
        "start_epoch = 0\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\" Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(start_epoch, 5):  # Train up to 5 epochs total\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for img_feat, input_ids, attn_mask in dataloader:\n",
        "        img_feat = img_feat.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attn_mask = attn_mask.to(device)\n",
        "\n",
        "        outputs = model(img_feat, input_ids, attn_mask)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\" Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # === Save checkpoint ===\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, checkpoint_path)\n",
        "    print(f\" Saved checkpoint at epoch {epoch}\")\n"
      ],
      "metadata": {
        "id": "YT20B-TtuL5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "VWkHSVv603ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "savePath='/content/drive/My Drive/AI Models/ImageCaptioning/Model'\n",
        "model.save_pretrained(savePath)\n",
        "tokenizer.save_pretrained(savePath)"
      ],
      "metadata": {
        "id": "Lnc_pBv_06ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate caption"
      ],
      "metadata": {
        "id": "0EZYIUlKqG1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelPath='/content/drive/My Drive/AI Models/ImageCaptioning/Model'\n",
        "model = ImageCaptioningModel.from_pretrained(modelPath)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(modelPath)\n",
        "\n",
        "def generate_caption(image_path, max_length=30):\n",
        "  model.eval()\n",
        "  img_feat = extract_image_features(image_path).unsqueeze(0).to(device)\n",
        "  prefix_embed = model.image_proj(img_feat).unsqueeze(1)\n",
        "\n",
        "  generated = torch.full((1, 1), tokenizer.bos_token_id, device=device, dtype=torch.long)\n",
        "  for _ in range(max_length):\n",
        "    gpt_embed = model.gpt2.transformer.wte(generated)\n",
        "    full_embed = torch.cat([prefix_embed, gpt_embed], dim=1)\n",
        "    attention_mask = torch.ones(full_embed.shape[:2], device=device)\n",
        "\n",
        "    outputs = model.gpt2(inputs_embeds=full_embed, attention_mask=attention_mask)\n",
        "    logits = outputs.logits[:, -1, :]\n",
        "    next_token = torch.argmax(logits, dim=-1).unsqueeze(1)\n",
        "    generated = torch.cat([generated, next_token], dim=1)\n",
        "    if next_token.item() == tokenizer.eos_token_id:\n",
        "        break\n",
        "  return tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "img = mpimg.imread('/content/drive/My Drive/Dataset/flickrDataset/images/1000268201_693b08cb0e.jpg')\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(generate_caption('/content/drive/My Drive/Dataset/flickrDataset/images/1000268201_693b08cb0e.jpg'))\n"
      ],
      "metadata": {
        "id": "MZ0GRhCWqIqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}